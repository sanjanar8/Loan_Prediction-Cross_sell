{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14769021,"sourceType":"datasetVersion","datasetId":9440233},{"sourceId":14769205,"sourceType":"datasetVersion","datasetId":9440365},{"sourceId":744063,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":567941,"modelId":580275},{"sourceId":744064,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":567942,"modelId":580276},{"sourceId":745411,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":569079,"modelId":581403}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-08T18:04:30.741359Z","iopub.execute_input":"2026-02-08T18:04:30.742118Z","iopub.status.idle":"2026-02-08T18:04:30.754694Z","shell.execute_reply.started":"2026-02-08T18:04:30.742085Z","shell.execute_reply":"2026-02-08T18:04:30.753698Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/trained-data/synthetic_crosssell_unsupervised_train.csv\n/kaggle/input/database-final/database.csv\n/kaggle/input/loan-sanction/scikitlearn/default/1/scaler (1).pkl\n/kaggle/input/loan-sanction/scikitlearn/default/1/label_encoders (1).pkl\n/kaggle/input/sklearn/scikitlearn/default/1/label_encoders.pkl\n/kaggle/input/sklearn/scikitlearn/default/1/glm.pkl\n/kaggle/input/sklearn/scikitlearn/default/1/lgbm_model.pkl\n/kaggle/input/sklearn/scikitlearn/default/1/feature_columns.pkl\n/kaggle/input/sklearn/scikitlearn/default/1/scaler.pkl\n/kaggle/input/sklearn/scikitlearn/default/1/le.pkl\n/kaggle/input/sklearn/scikitlearn/default/1/knn.pkl\n/kaggle/input/keras/keras/default/1/config.json\n/kaggle/input/keras/keras/default/1/metadata.json\n/kaggle/input/keras/keras/default/1/model.weights.h5\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import re\nimport statsmodels.api as sm\nimport joblib\nfrom tensorflow.keras.models import load_model\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T18:04:09.538036Z","iopub.execute_input":"2026-02-08T18:04:09.538447Z","iopub.status.idle":"2026-02-08T18:04:30.718069Z","shell.execute_reply.started":"2026-02-08T18:04:09.538373Z","shell.execute_reply":"2026-02-08T18:04:30.717084Z"}},"outputs":[{"name":"stderr","text":"2026-02-08 18:04:14.275220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770573854.541319      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770573854.611767      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770573855.244228      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770573855.244332      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770573855.244335      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770573855.244338      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"smoothing={'Advertising': np.float64(0.08156557675300376),\n 'Agriculture': np.float64(0.10462958124779571),\n 'Bank': np.float64(0.05196952252465978),\n 'Business Entity Type 1': np.float64(0.08138259729639118),\n 'Business Entity Type 2': np.float64(0.08527949334417956),\n 'Business Entity Type 3': np.float64(0.09299443087254153),\n 'Cleaning': np.float64(0.11039736368358759),\n 'Construction': np.float64(0.11674450872003694),\n 'Culture': np.float64(0.05605986682408393),\n 'Electricity': np.float64(0.06646592520267568),\n 'Emergency': np.float64(0.07159173367468184),\n 'Government': np.float64(0.06979136625644024),\n 'Hotel': np.float64(0.06435172970754985),\n 'Housing': np.float64(0.07944989494426169)\n }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-08T18:04:42.696931Z","iopub.execute_input":"2026-02-08T18:04:42.697253Z","iopub.status.idle":"2026-02-08T18:04:42.704103Z","shell.execute_reply.started":"2026-02-08T18:04:42.697227Z","shell.execute_reply":"2026-02-08T18:04:42.702999Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"### LoanSanction Class\n\nA class that loads the saved LGBM model + label encoders + scaler and performs:\n\nTarget encoding for ORGANIZATION_TYPE\n\nCategorical label encoding\n\nScaling numerical features\n\nPredicting loan sanction result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LoanSanction:\n    def __init__(self, model_path: str, training_data_path: str, smoothing):\n        self.model_path = model_path\n        self.training_data_path = training_data_path\n        self.models = {}\n        self.smoothing = smoothing\n        self._load_models()\n        \n    def _load_models(self):\n        self.models['label_encoder'] = joblib.load(f\"{self.model_path}/label_encoders (1).pkl\")\n        self.models['Scaler'] = joblib.load(f\"{self.model_path}/scaler (1).pkl\")\n        self.models['lgbm'] = joblib.load(f\"{self.model_path}/lgbm_model.pkl\")\n        \n    def _Encode(self, inputs: pd.DataFrame):\n        inputs['ORGANIZATION_TYPE_encoded'] = inputs['ORGANIZATION_TYPE'].map(self.smoothing)\n        categorical_cols = [c for c in inputs.columns if inputs[c].dtype == 'object']\n        for col in categorical_cols:\n            inputs[col] = self.models['label_encoder'][col].transform(inputs[col].astype(str))\n        return inputs\n        \n    def _predict(self, d: dict) -> float:\n        inputs = pd.DataFrame([d])\n        inputs = self._Encode(inputs)\n        numerical_cols = [c for c in inputs.columns \n                          if inputs[c].dtype in ['int64','float64'] and c != 'ORGANIZATION_TYPE_encoded']\n        inputs[numerical_cols] = self.models['Scaler'].transform(inputs[numerical_cols])\n        inputs = inputs.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x))\n        pred = self.models['lgbm'].predict(inputs)\n        return pred\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CustomerInferenceEngine Class\n\nResponsible for:\n\nCross-sell recommendations\n\nNearest neighbor lookup\n\nSpend prediction using GLM\n\nIntegrating LoanSanction as pre-check","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\nclass CustomerInferenceEngine:\n    def __init__(self, model_path: str, training_data_path: str):\n        self.model_path = model_path\n        self.training_data_path = training_data_path\n        self.data_base_path = \"/kaggle/input/database-final/database.csv\"\n        self.models = {}\n        self._load_models()\n        self._load_training_data()\n        self._load_data_base()\n\n    def _load_models(self):\n        self.models[\"encoder\"] = load_model(f\"{self.model_path}/encoder\")\n        self.models[\"scaler\"] = joblib.load(f\"{self.model_path}/scaler.pkl\")\n        self.models[\"knn\"] = joblib.load(f\"{self.model_path}/knn.pkl\")\n        self.models[\"label_encoders\"] = joblib.load(f\"{self.model_path}/label_encoders.pkl\")\n        self.models[\"feature_columns\"] = joblib.load(f\"{self.model_path}/feature_columns.pkl\")\n        self.models[\"glm\"] = joblib.load(f\"{self.model_path}/glm.pkl\")\n        self.models[\"loan_encoder\"] = joblib.load(f\"{self.model_path}/le.pkl\")\n\n    def _load_training_data(self):\n        df = pd.read_csv(self.training_data_path)\n        if \"Unnamed: 0\" in df.columns:\n            df = df.drop(columns=[\"Unnamed: 0\"])\n        self.training_data = df\n\n    def _load_data_base(self):\n        df = pd.read_csv(self.data_base_path)\n        if \"Unnamed: 0\" in df.columns:\n            df = df.drop(columns=[\"Unnamed: 0\"])\n        self.data_base = df\n\n    def _encode_common_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        for col in [\"PREFERRED_CHANNEL\", \"LAST_CAMPAIGN_RESPONSE\"]:\n            if col in df.columns:\n                df[col] = self.models[\"label_encoders\"][col].transform(df[col].astype(str))\n        return df\n\n    def _find_similar_customers(self, customer_data: pd.DataFrame, k: int = 10):\n        \n        df = customer_data.drop(columns=[\"CURRENT_LOAN_OWNED\", \"Value_12M\"], errors=\"ignore\")\n        df = self._encode_common_features(df)\n        df = df[self.models[\"feature_columns\"]]\n\n        scaled = self.models[\"scaler\"].transform(df)\n        embedding = self.models[\"encoder\"].predict(scaled, verbose=0)\n\n        distances, indices = self.models[\"knn\"].kneighbors(embedding, n_neighbors=k)\n\n        neighbors = []\n        for i, idx in enumerate(indices[0]):\n            row = self.training_data.iloc[int(idx)]\n            neighbors.append({\n                \"rank\": i + 1,\n                \"neighbor_index\": int(idx),\n                \"distance\": float(distances[0][i]),\n                \"similarity_score\": float(1 / (distances[0][i] + 1e-6)),\n                \"product\": row.get(\"CURRENT_LOAN_OWNED\")\n            })\n\n        return neighbors\n\n    def _recommend_products(self, neighbors):\n        product_scores = {}\n\n        for n in neighbors:\n            product = n.get(\"product\")\n            if pd.notna(product):\n                if product not in product_scores:\n                    product_scores[product] = {\"count\": 0, \"total_similarity\": 0.0}\n                product_scores[product][\"count\"] += 1\n                product_scores[product][\"total_similarity\"] += n[\"similarity_score\"]\n\n        for product, info in product_scores.items():\n            info[\"avg_similarity\"] = info[\"total_similarity\"] / info[\"count\"]\n\n        return sorted(product_scores.items(), key=lambda x: x[1][\"total_similarity\"], reverse=True)\n\n    def _predict_spend(self, customer_data: pd.DataFrame) -> float:\n        \n        df = customer_data.drop(columns=[\"CURRENT_LOAN_OWNED\", \"Value_12M\"], errors=\"ignore\")\n        df = self._encode_common_features(df)\n\n        df[\"CURRENT_LOAN_OWNED\"] = self.models[\"loan_encoder\"].transform(df[\"CURRENT_LOAN_OWNED\"].astype(str))\n\n        df = df.drop(columns=[\"MONTHLY_SPEND\"], errors=\"ignore\")\n        df.insert(0, \"const\", 1.0)\n\n        glm_cols = list(self.models[\"glm\"].params.index)\n        df = df[glm_cols]\n\n        return float(self.models[\"glm\"].predict(df)[0])\n\n    def _predict(self, customer_data: dict) -> dict:\n        \n        ls = LoanSanction(self.model_path, self.training_data_path, smoothing=None)\n\n        pred = ls._predict(customer_data)\n\n        if pred != 1:\n            return {\"customer_id\": customer_data[\"SK_ID_CURR\"], \"loan_approved\": False}\n\n        val = customer_data[\"SK_ID_CURR\"]\n\n        if val not in self.data_base['SK_ID_CURR'].values:\n            return {\"customer_id\": val, \"loan_approved\": True, \"error\": \"Customer not found in database\"}\n\n        df = self.data_base[self.data_base[\"SK_ID_CURR\"] == val].copy()\n\n        neighbors = self._find_similar_customers(df)\n        recommendations = self._recommend_products(neighbors)\n        predicted_spend = self._predict_spend(df)\n\n        return {\n            \"customer_id\": customer_data[\"SK_ID_CURR\"],\n            \"current_product\": customer_data.get(\"CURRENT_LOAN_OWNED\"),\n            \"predicted_monthly_spend\": predicted_spend,\n            \"neighbors\": neighbors[:5],\n            \"recommendations\": recommendations[:5],\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}